%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% TODO
% After finishing with the paper and supplementary, name all figures and sections as they are in the paper and in the supplementary materials with numbers, etc.
% make the captions in normal size
% check that all capitalizations in titles are the same
% quotes should be in a different text color
% לבדוק איך צריך להכניס משוואות בסופלמנטרי

\documentclass[12pt]{article}
% Increase margin width for todos
\setlength{\marginparwidth}{2cm}
\usepackage{comment}
\usepackage{geometry}
\geometry{a4paper, margin=0.6in}
\usepackage{setspace}
\usepackage{amsmath}
\usepackage[most]{tcolorbox}
\usepackage{pifont}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{capt-of}
\usepackage{caption}
\usepackage{soul}           % For text highlighting
\usepackage{xcolor}         % For color definitions
\usepackage{todonotes}      % For margin notes
\usepackage{enumitem,amssymb}
\usepackage{bm}% bold math
\usepackage{enumitem}
\usepackage{float}


% Define custom colors
\definecolor{softtodogreen}{HTML}{90EE90}  % Light green
\setuptodonotes{color=softtodogreen}

\setstretch{1.5}

% Define different colored highlighting commands
\newcommand{\hlgreen}[1]{\sethlcolor{green!30}\hl{#1}}
\newcommand{\hlred}[1]{\sethlcolor{red!30}\hl{#1}}
\newcommand{\hlyellow}[1]{\sethlcolor{yellow!50}\hl{#1}}
\newcommand{\hlblue}[1]{\sethlcolor{blue!20}\hl{#1}}

% Define custom colors
\definecolor{commentcolor}{RGB}{150, 0, 0}
\definecolor{solved_commentcolor}{RGB}{0, 150, 0}
\definecolor{responsecolor}{RGB}{0, 90, 150}


\newcounter{reviewpoint}
\setcounter{reviewpoint}{0}

% For the numbered reviewer points
\newcommand{\reviewpoint}{%
  \stepcounter{reviewpoint}%
  \vspace{0.5em}%
  {\noindent\textbf{\thereviewpoint.}}%
  \vspace{-1.2em}% Adjust this value to control spacing between number and box
}



\usepackage{mdframed}

\newenvironment{coloredquote}{
    \begin{mdframed}[
        linewidth=0pt,
        leftline=true,
        rightline=false,
        topline=false,
        bottomline=false,
        linecolor=quotationcolor,
        backgroundcolor=quotationcolor!5,
        leftmargin=0pt,
        rightmargin=0pt,
        innerleftmargin=10pt,
        innerrightmargin=5pt,
        innertopmargin=5pt,
        innerbottommargin=5pt,
        font=\itshape
    ]
}{
    \end{mdframed}
}


% For a box-style quote using tcolorbox (which you already have)
\newtcolorbox{bluebox}{
  colback=blue!5,
  colframe=blue!40,
  boxrule=0.5pt,
  arc=2mm,
  left=6pt,
  right=6pt,
  top=3pt,
  bottom=3pt
}
    
% Define custom environments with proper alignment
\newenvironment{reviewercomment}
    {\begin{tcolorbox}[width=\linewidth,colback=gray!5,colframe=commentcolor!50,title=Reviewer Comment,left=5pt,right=5pt]}
    {\end{tcolorbox}}

\newenvironment{solved_reviewercomment}
    {\begin{tcolorbox}[width=\linewidth,colback=gray!5,colframe=solved_commentcolor!50,title=Reviewer Comment,left=5pt,right=5pt]}
    {\end{tcolorbox}}
    
\newenvironment{ourresponse}
    {\begin{tcolorbox}[width=\linewidth,breakable,enhanced,colback=gray!5,colframe=responsecolor!50,title=Response,left=5pt,right=5pt]}
    {\end{tcolorbox}}


% For todos, clean definition without duplicates
\newlist{todolist}{itemize}{2}
\setlist[todolist]{label=$\square$}



\begin{document}

\begin{itemize}

\item[\rlap{\raisebox{0.3ex}{\hspace{0.4ex}\scriptsize \ding{51}}}$\square$]
Coherent spot size simulation: achieve more stastics and try to solve it

\item[\rlap{\raisebox{0.3ex}{\hspace{0.4ex}\scriptsize \ding{51}}}$\square$]
\st{Analytic} dependence on M (covariance estimator + rank of object) 

\item[\rlap{\raisebox{0.3ex}{\hspace{0.4ex}\scriptsize \ding{51}}}$\square$]
Phase retrieval: (a) number of iterations (show plateau) (b) noise + number of realizations (needs good approximation of AC)

\item[\rlap{\raisebox{0.3ex}{\hspace{0.4ex}\scriptsize \ding{51}}}$\square$]
Show convergence graph in some measurements + heatmap of iterations as function of M / function of sparsity

\item[\rlap{\raisebox{0.3ex}{\hspace{0.4ex}\scriptsize \ding{51}}}$\square$]
\st{Modify [Kang et al., Nat. Commun., 2023] and if possible try and simulate such solution}

\item[$\square$]
Mosaicing experiment

\item[$\square$]
SVD and show depndence of singular vector and energy conservation

\item[\rlap{\raisebox{0.3ex}{\hspace{0.4ex}\scriptsize \ding{55}}}$\square$]
Figure of different multiplicative factors and their dependence + Explain more the multiplication of different constants 

\item[\rlap{\raisebox{0.3ex}{\hspace{0.4ex}\scriptsize \ding{51}}}$\square$]
Heatmap of sparsity and M with noise

\item[\rlap{\raisebox{0.3ex}{\hspace{0.4ex}\scriptsize \ding{51}}}$\square$]
Heatmap of sparsity and M

\end{itemize}
\newpage
\title{Response to Reviewers}


\section{Reviewer \#1 Comments (from PDF):}

In this work, the authors extend the matrix approach of optical imaging they have been developing with other groups for the last few years to the context of dynamic scattering media. This topic is of great interest because matrix imaging has shown its great potential for overcoming multiple scattering in the long-standing quest towards deep imaging of complex media such as biological tissues. However, it relies by essence on the hypothesis of a static medium. Their approach relies on the fact that, under the isoplanatic hypothesis, dynamic scattering is mathematically analogous to dynamic illumination in static scattering media. From this analogy, they are then able to use the methods developed in previous papers [13] for scattering compensation under dynamic illumination in which the covariance matrix of the reflected wave-field was investigated rather than just the reflection matrix itself. This covariance matrix has already been recently leveraged to extend the scope of matrix imaging to incoherent (fluorescent) imaging by the authors themselves [14]. This paper is a new important demonstration of the versatility and the universality of matrix approaches to wave imaging in complex media. For incoherent imaging, the demonstration of the superiority of matrix imaging with respect to speckle correlation techniques [28,29] is convincing and might have deserved more publicity than just a short section in the Supplementary Material. For coherent imaging applications, the hypothesis of a constant incident wave-field is a bit contradictory with the problem of dynamic scattering and would deserve more discussion.  

As often with this group, this work is technically of excellent quality and its presentation is elegant. The experiments based on rotating diffusers are rather simple and allow the authors to illustrate nicely the concept. However, one can wonder whether the idea proposed by the authors can go beyond imaging through a thin dynamic diffusive layer and to which extent it can be applied to real 3D imaging situations like for biomedical or LiDAR applications. I detail below the main points that, in my opinion, would deserve more discussion before publication.

\begin{ourresponse}
    % We thank the Reviewer for their general positive evaluation of our work, recognizing its significance in extending matrix-based imaging to dynamic scattering media and its potential impact. We agree that the important question on the effects of thick scattering media and applications in coherent imaging deserves more discussion. We address it in our point-by-point replies below.
    We thank the Reviewer for their general positive evaluation of our work, recognizing its significance in extending matrix-based imaging to dynamic scattering media and its potential impact. We appreciate the reviewer's assessment of our work as "a new important demonstration of the versatility and the universality of matrix approaches to wave imaging in complex media" and have incorporated this insight into our conclusion. We agree that the important question on the effects of thick scattering media and applications in coherent imaging deserves more discussion. We address it in our point-by-point replies below.
\end{ourresponse}


\begin{enumerate}[label=\arabic*.]
\item \leavevmode
\end{enumerate}
\vspace{-1em}
\begin{solved_reviewercomment}
    For instance, in the coherent imaging experiment depicted in Fig.~\ref{fig_5}, the approach requires a constant incident wave-field, which is complicated if one wants to image through a dynamic scattering medium. In the present experiment, the authors circumvent this issue by focusing the incident wave-field in the diffuser plane such that the beam waist is contained in one coherence length of the diffuser. However, in practical situations, the scattering medium is three-dimensional, and the condition of a constant incident wave-field becomes impossible to fulfill. In real life, dynamic scattering impacts both the incident and reflected wave-fields. The authors do not discuss really this apparent contradiction between the assumption of a constant incident wave-field and dynamic scattering that will break this invariance.
\end{solved_reviewercomment}

\begin{ourresponse}
    We thank the referee for raising this point, which was not sufficiently emphasized in the original manuscript. 
We recognize that the approach applied for thin scattering layers would indeed not be as effective for volumetric scattering media, where maintaining an effective constant illumination may become challenging.
    
    To address this limitation transparently in our manuscript, we have made the following three revisions to our manuscript: first, we added the following clarification to the Discussion Section:
    
    \begin{quote}
        "We note that for coherent imaging through dynamic scattering, our current approach is limited to scenarios where effectively constant illumination can be maintained at the object plane. While demonstrated here with a thin scattering layer by focusing the illumination to a spot that is smaller than the coherence area of the scattering layer, thick volumetric scattering presents a challenge that requires additional or alternative solutions."
    \end{quote}
     Secondly, we suggest a potential mitigation technique for obtaining an effectively homogeneous illumination in the case of coherent illumination through thick media, while maintaining the capability of coherence gating, crucial for practical reflection-based microscopy and 3D imaging, and highlight the potential of focused illumination in imaging through dynamically-bent multi-core fibers (MCFs):

     \begin{quote}
     "
    One potential solution for obtaining an effectively homogeneous illumination in the case of coherent illumination through thick dynamic media, while maintaining the capability of coherence gating, without requiring focused illumination, is by digital incoherent compounding the time-gated frames of several illuminations taken within the correlation time of the medium. 
    In such a solution, for each realization of the dynamic scatterer one would: (I) rapidly acquire multiple ($K \gg 1$) coherence-gated holograms under different speckle illuminations (created with an additional diffuser or SLM in the illumination path); (II) Incoherently sum the intensity patterns of the holographically measured fields to incoherently-compounded "macro-frames": $I_m(x,y,z=z_{obj}) = \sum_{k=1}^{K}|E_{m,k}(x,y,z=z_{obj})|^2$  that can then be (III) processed using I-CLASS as in our incoherent experiments.
    This protocol creates effectively uniform illumination through incoherent compounding, while preserving the important coherence-gating capability of coherent light that is crucial for practical reflection-based microscopy and 3D-imaging . The $K$ rapid illumination patterns can be random or complementary speckle illuminations that would result in more homogeneous illumination distribution \cite{gateau2017complementary}.

    Such a hybrid method offers a potential mitigation strategy for thick scattering media at the price of an increase in the number of acquisitions, or equivalently, at the price of acquisition or dynamic scattering speed. We numerically study the required number of additional acquisitions on the reconstruction quality in Supplementary Section XX.
    %forward for complex volumetric media while maintaining the benefits of coherent imaging.
    Additionally, we highlight the applicability of the tight focusing approach to lensless imaging through highly dynamic flexible multi-core fiber (MCF) endoscopes, as previously demonstrated only through relatively static fibers \cite{choi2022flexible, haim2025image}. In this system, uniform illumination can be obtained by single-mode excitation of a single fiber core in a relatively straightforward fashion \cite{weinberg2024ptychographic}.
    \end{quote}


Thirdly, we have performed two numerical studies on the effects of imperfect (in)homogeneous illumination on reconstruction fidelity in the case of coherent imaging. One of the studies focuses on the case of a single coherent illumination with a focused beam, and the other on the hybrid incoherent-compounding approach suggested above.
The results of these studies are presented in the new Supplementary Sections XX,YY, which reads:

    \begin{quote}
    \section*{Supplementary Section SXX: Effect of illumination spot size and homogeneity on coherent imaging through dynamic scattering}

In our coherent imaging demonstration (Fig.~\ref{fig_5} of the main text), maintaining a constant homogeneous illumination pattern at the object plane despite the dynamic scattering introduced by the rotating diffuser is crucial for the successful application of our reconstruction algorithm (Eqs. 6-7 of the main text). %, our method requires that the illumination field $E_m^{ill}(\vec{r})$ remains effectively constant across different acquisitions for our method to work properly.
While obtaining a constant illumination through a dynamic scatterer is often challenging, this can be acheived for the case of a dynamic \textit{thin} scatterer, such as a diffuser, by focusing the illumination spot size on the diffuser to be sufficiently smaller than the  diffuser's coherence (or correlation) length.
Fig.~\ref{fig_S7} demonstrates and studies the limitation of this approach through numerical simulation. The top row (Fig.~\ref{fig_S7}a-c) shows the phase pattern of the diffuser with the illumination spot superimposed for three cases: where the spot size is 0.1, 1, and 10 times the diffuser correlation length ($d_{\text{correlation}}$), respectively. The second row (Fig.~\ref{fig_S7}d-f) shows the resulting illumination intensity pattern at the object plane after the light has propagated through the diffuser, simulated using Fourier transform propagation.
The third row (Fig.~\ref{fig_S7}g-i) shows the effect of these illumination patterns on the effective object (the product of the object and the illumination), while the fourth row (Fig.~\ref{fig_S7}j-l) shows the reconstruction results. 




\begin{figure}[H]
\centering
\includegraphics[width=0.99\textwidth]{figures/figure_S7.pdf}
\renewcommand{\thefigure}{S7}
\captionof{figure}{\footnotesize\textbf{Effect of illumination spot size on imaging through dynamic scattering.} 
Numerical simulations demonstrating how the ratio of illumination spot size ($d_{spot}$) to diffuser correlation length ($d_{corr}$) affects illumination at the object plane and reconstruction quality. Each column represents a different spot size ratio: $\frac{d_{spot}}{d_{corr}}=0.1$ (left), $\frac{d_{spot}}{d_{corr}}=1$ (middle), and $\frac{d_{spot}}{d_{corr}}=10$ (right). \textbf{a-c} Illumination phase pattern on the diffuser surface. The colorbar indicates phase values from $-\pi$ to $\pi$. The white inset in (a) shows the zoomed-in beam spot. \textbf{d-f} Resulting intensity patterns at the object plane after numerical propagation through the diffuser. The grayscale colorbar indicates normalized intensity. \textbf{g-i} Effective object (target object multiplied by the illumination pattern). \textbf{j-l} Reconstructed images after applying the I-CLASS algorithm. Note how a small spot size relative to the diffuser correlation length (left column) maintains relatively uniform illumination at the object plane, enabling high-quality reconstruction, while larger spot sizes (middle and right columns) create varying speckle patterns that degrade reconstruction quality.}
\label{fig_S7}
\end{figure}

When the illumination spot is much smaller than the diffuser's correlation length (Fig.~\ref{fig_S7}a), it effectively samples only a single "phase patch" of the diffuser. As the diffuser rotates between realizations, this results in a relatively uniform illumination at the object plane that experiences only a global phase shift but maintains its spatial intensity pattern across different realizations. This consistency across different diffuser positions is crucial for our method to work properly. In other words, while the global phase of the illumination might change, its spatial distribution pattern remains effectively identical from one diffuser position to the next, satisfying our requirement for constant spatial illumination.

In contrast, when the illumination spot size is comparable to or larger than the diffuser correlation length (Fig.~\ref{fig_S7}c), the beam simultaneously samples multiple uncorrelated regions of the diffuser. This produces complex speckle patterns at the object plane that vary significantly between diffuser positions, creating a different illumination pattern for each diffuser realization. This variation violates our assumption of constant illumination in Eq. 6-7 of the main text. As a result, the reconstruction quality clearly degrades as the spot size increases relative to the diffuser correlation length, demonstrating the importance of maintaining consistent illumination across different diffuser realizations.


In our experimental implementation described in the main text, we carefully focused the beam to ensure a spot size smaller than the diffuser's correlation length (which was $\sim 70 \mu m$), thereby maintaining sufficiently constant spatial illumination patterns at the object plane across different realizations, as required.
\end{quote}

\end{ourresponse}


\begin{enumerate}[label=\arabic*., resume]
\item \leavevmode
\end{enumerate}
\vspace{-1em}
\begin{solved_reviewercomment}
    Another issue of three-dimensional scattering is that it breaks the isoplanatic assumption on which the current approach relies. The authors mention in the discussion the idea of mosaicking the field-of-view but it somehow requires a roughly focused incident wave-field and only applies to moderate scattering. Another route is a multi-conjugate adaptive optics scheme in which multiple scattering trajectories are rectified by compensating aberrations from a stack of phase screens conjugated with different planes inside the scattering medium [Kang et al., Nat. Commun., 2023]. Would the current approach proposed by the authors be compatible with such a multi-conjugate strategy? It might be relevant to discuss this point in the conclusion.
\end{solved_reviewercomment}

\begin{ourresponse}
    We thank the reviewer for highlighting this important limitation. We have indeed demonstrated reconstruction only under isoplanatic conditions in our current proof-of-principle. The extension to thick scattering media represents an important challenge and direction for future research.

    Indeed, in conventional reflection-matrix-based imaging or computational wavefront shaping, thick scatterers can be addressed by either mosaicking, in the case of moderate scattering, or using multi-conjugate approaches leveraging a multi-layer scattering model. However, while mosaicking can still be similarly applied on our datasets, our approach presents a unique twist: in our formulation, the dynamic medium plays the role that would normally be occupied by the object in conventional reflection-matrix implementation, and vice versa. This roles reversal means that the multi-conjugate, multi-slice approach that was recently effectively applied for computational scattering correction would now address a thick target object rather than a thick medium. Addressing a thick scattering medium in our case is thus analogous to addressing a thick target object in conventional reflection matrix works. Interestingly, in this respect, the modeling and treatment of thick targets proposed in the recent work by Park et al. \cite{oh2025digital} may offer a pathway forward. In this work, the authors present a technique that allows for handling thick target objects rather than treating them as purely planar. 

    Alternative solutions may also be derived from the model-based gradient descent optimization approach recently reported by Haim et al. \cite{haim2025image}. This method's flexibility in optimizing a high number of parameters in a model may be utilized to integrate the case of a thick medium (and thick object).
    
    To address this point in the revised manuscript, we have added the following paragraph to the discussion section:
    \begin{quote}
        "While we have focused our proof-of-principle demonstrations on isoplanatic scattering conditions, extending our approach to thick dynamic scattering media remains an important challenge. Beyond the direct application of mosaicking approaches, which are effective for moderate scattering \cite{lee22, najar2024harnessing}, applying a multi-conjugate, "multi-slice" correction \cite{kang2023tracing, haim2025image} would be very attractive. However, since in our formulation the dynamic medium plays the role that would normally be occupied by the object in conventional reflection-matrix implementation, and vice versa, the conventional multi-conjugate approach would only address a thick target object rather than a thick scattering medium. Addressing a thick scattering medium thus requires a solution analogous to addressing a thick target object in conventional reflection matrix imaging. Interestingly, the recent approach of Park et al. \cite{oh2025digital}, where thick target objects are considered, may offer a potential path forward. 
        Alternatively, it may be possible to leverage the recent model-based gradient descent optimization approach  \cite{haim2025image}, which can flexibly handle a multi-parameter model, to integrate the case of a thick medium in the dynamic measurements formalism. 
        %Recent advances in modeling thick targets rather than planar ones, such as those presented by Park et al. \cite{oh2025digital}, suggest potential pathways for adaptation to our scenario, where the dynamic medium mathematically plays the role of the object. Future work could explore combining our approach with multi-conjugate modeling, treating thick media as a series of thin scattering layers at different depths, each contributing its own phase and amplitude distortions to the overall scattering process."
    \end{quote}
    
\end{ourresponse}

\begin{enumerate}[label=\arabic*., resume]
\item \leavevmode
\end{enumerate}
\vspace{-1em}
\begin{solved_reviewercomment}
    One last point which is not really discussed in the paper is the number of required independent frames \(M\) to converge towards a reliable estimation of the object, or, in other words, the scaling of the estimator bias with respect to this number \(M\). An analytical formulation of the problem could be performed to derive this number, which is important for future implementations of the method. I guess this number will depend on the object complexity/sparsity.
\end{solved_reviewercomment}
    
\begin{ourresponse}
    We appreciate the reviewer's important question about the number of required independent frames for reliable estimation. To address this systematically, we conducted a comprehensive set of numerical simulations investigating the relationship between the number of realizations (M), the measurement signal-to-noise ratio, the object sparsity, and the reconstruction fidelity. We present the results of this study in a new supplementary section ("Dependence of reconstruction quality on number of realizations, object sparsity, and SNRR").


    
    
    % \hlred{Take from I-CLASS / Acusto-optic the paragraph about the theoretical dependence on M which is logarithmic (from the CTR paper) and add that there are additional factors to this theory such as the object complexity and SNR}.
    
    \begin{quote}
        \section*{Dependence of reconstruction quality on number of realizations, object sparsity, and SNR}

        % \hlred{I suggest inserting something about how statistical siginficant is this result? how many experiments are in each heatmap cell? and what is the std between expereimnts (you have 3 random variables, the random object, the shot noise and the PSFs, maybe freeze 2 each time and run it to understand if they have the same effect}.

        
        To study the dependence of the reconstruction quality on the number of realizations, object complexity, and SNR, we carried an in-depth numerical study where we numerically simulate reconstructions using a different number of random realizations (M) for different object complexities (as given by the object sparsity) for both incoherent and coherent imaging scenarios.
        The results of these analyses are presented as reconstruction fidelity heatmaps in Fig.~\ref{fig_S8}
        To ensure statistical robustness, each data point in these heatmaps represents the average outcome from 10 independent numerical experiments.

        
        \begin{figure}[H]
            \centering
            \includegraphics[width=0.93\textwidth]{figures/figure_S8.pdf}
            \renewcommand{\thefigure}{S8}
            \captionof{figure}{\footnotesize\textbf{Reconstruction fidelity as a function of number of realizations and object sparsity.} 
            Cross-correlation scores between reconstructed and ground truth images are shown as heatmaps for different imaging scenarios. \textbf{a} Example simulated objects with different sparsity levels: $2^{12}$ (left) and $2^8$ (right) bright points. \textbf{b} Incoherent imaging results under different SNR conditions: SNR=5 (left), SNR=10 (middle), and SNR=$\infty$ (right). \textbf{c} Coherent imaging results under the same SNR conditions: SNR=5 (left), SNR=10 (middle), and SNR=$\infty$ (right). The horizontal axis shows the number of realizations (M), and the vertical axis shows object sparsity (represented as the number of bright points). Color scale indicates correlation coefficient from 0 to 1. Each data point represents the average result from 10 distinct numerical experiments with different random objects under identical parameters. For all simulations, a camera pixel count of N=350$^2$ was used.}
            \label{fig_S8}
        \end{figure}

        These results demonstrate that sparser objects can be reconstructed with fewer realizations, while complex objects need substantially more measurements. Higher SNR conditions predictably improve reconstruction quality, and coherent imaging appears to require an increased number of measurements compared to incoherent imaging for an equivalent fidelity.

                   
    \end{quote}

        We note that the theoretical analysis of CTR-CLASS convergence, as established in previous work by Lee et al. \cite{lee22}, indicates a logarithmic dependence of the required number of measurements on the degrees of freedom (i.e., the number of camera pixels $N$). However, this theoretical scaling is modified in practice by additional factors, including object complexity (sparsity) and signal-to-noise ratio (SNR). These factors play important roles in determining the practical minimum number of frames needed for successful reconstruction.


\end{ourresponse}



\textbf{More Specific Comments:}


\begin{enumerate}[label=\arabic*., resume]
\item \leavevmode
\end{enumerate}
\vspace{-1em}
\begin{solved_reviewercomment}
    The supplementary Section S2 that shows the impact of correlations between speckle patterns is very interesting and might have deserved a place in the main manuscript. In this Supplementary Section, these correlations arise from energy conservation (phase-distortions). Maybe a singular value (or equivalently eigenvalue) decomposition of the covariance matrix \cite{badon2020distortion}(citation [10] in manuscript) can help in that case? The first singular vector would be less affected by these negative correlations than the CLASS algorithm. Can the authors comment on that? 
\end{solved_reviewercomment}

\begin{ourresponse}
    \begin{comment}
        \hlred{But citation [10]  doesn't perform SVD on the covariance matrix: A. it is not on the covariance but on the full matrix, and B. it is also not on the matrix but on the 'de-scanned' / 'distortion' matrix, which is different. This cannot apply here where the energy conservation creates a problem in the covariance calculation, **before applying algorithm**}

        \hlred{Maybe refer to Wonshik's work on SVD (the Science Adavnces one https://www.science.org/doi/10.1126/sciadv.abo4366) and show him the effect of pre-processing the measurement matrix with SVD filtering, create a graph where the x axis is the number of non-zero singular values, and the correlation to GT, and see if there is a value smaller than M that will give better correlation}
    \end{comment}
    

    We thank the reviewer for this insightful comment. Following this recommendation, we have added the following discussion to the main manuscript:
    
    \begin{quote}
        Our analysis revealed that energy conservation in phase-only speckle patterns introduces troublesome spatial correlations that can cause background haze in the reconstructions. We addressed this by simply applying varying intensity scaling across frames during post-processing. It would be interesting to study whether a singular value decomposition filtering (SVD) of the measurement matrix or the distortion matrix \cite{badon2020distortion,jo2022through} can help to address these effects .
        
        % It was recently shown that SVD-based filtering of the distortion matrix \cite{badon2020distortion} or the reflection matrix \cite{jo2022through} can XXXXX. An in-depth study of the potential of advnaced filtering of the measurement or covariance matrix will be the focus of future work.

        It was recently shown that SVD-based filtering of the distortion matrix \cite{badon2020distortion} or the reflection matrix \cite{jo2022through} can help isolate single-scattering contributions from complex systems with multiple scattering. In the distortion matrix approach \cite{badon2020distortion}, the first singular vectors capture the most XXXX signal contributions, while in \cite{jo2022through}, they showed that eigenvalue decomposition of a time-gated reflection matrix can effectively attenuate uncorrelated multiple scattering while retaining single-scattering signals with strong wave correlation. An in-depth study of the potential of advanced filtering of the measurement or covariance matrix will be the focus of future work.

        
        The full analysis of the effects of the energy-conservation originated correlations on the reconstruction, and our approach for mitigating them, is detailed in Supplementary Section S2 and Supplementary Figure S3.
    \end{quote}
    
    We maintained the detailed analysis in Supplementary Section S2 as you suggested, while bringing the key insight to readers' attention in the main text.

\end{ourresponse}


    
\begin{enumerate}[label=\arabic*., resume]
\item \leavevmode
\end{enumerate}
\vspace{-1em}
\begin{solved_reviewercomment}
    The authors show how to circumvent this issue by a multiplication of each frame by a fixed scalar value. This is interesting, but the choice of the multiplication factor from one to two lacks justification. Can the authors provide more details about this choice? 
\end{solved_reviewercomment}

\begin{ourresponse}
We thank the reviewer for raising this point, which was not properly addressed in the original manuscript. To address this, we added to the revised manuscript a systematic study examining how the choice of modulation depth affect reconstruction quality. The results of this study have been added to the Supplementary Materials section discussing the amplitude modulation.



\begin{quote}

As discussed above, we addressed the off-diagonal correlations in the covariance matrix that are introduced by energy conservation by applying variable intensity modulation (scaling) across the captured frames. This was done by simply multiplying each captured frame by a scalar factor. We empirically found that modulating by factors between 1 and 2 gave near-optimal results. In Fig.~\ref{fig_S8} we present the reconstruction fidelity as a function of the choice of modulation depth for the experimental results of ~\ref{fig_2} in the main manuscript. To generate this result, we have performed multiple reconstructions on the same experimental dataset, where before each reconstruction, a different value of varying amplitude modulation (scaling) was applied. For each depth value, we applied a linear scaling to our experimental data where the multiplication factor for the $m$-th frame (out of $M$ total frames) was:


\begin{equation}
    f_m = 1 + \frac{m-1}{M-1} \cdot (\alpha -1) = 1, 2, 3, ..., \alpha
\end{equation}

Where $\alpha$ controls the total range of modulation with corresponding modulation depth: $\frac{max(f_m)-min(f_m)}{max(f_m)+min(f_m)}=\frac{\alpha-1}{\alpha+1}$.

\begin{comment}
    We can rewrite this equation directly in terms of the modulation depth $d$ by solving for $\alpha$:

    \begin{align}
    d &= \frac{\alpha-1}{\alpha+1} \\
    d(\alpha+1) &= \alpha-1 \\
    d\alpha + d &= \alpha - 1 \\
    d\alpha - \alpha &= -1 - d \\
    \alpha(d-1) &= -1 - d \\
    \alpha &= \frac{-1-d}{d-1} = \frac{1+d}{1-d}
    \end{align}

    Substituting this into our original equation:
    
    \begin{align}
    f_m &= 1 + \frac{m-1}{M-1} \cdot \left(\frac{1+d}{1-d} - 1\right) \\
    &= 1 + \frac{m-1}{M-1} \cdot \frac{1+d-(1-d)}{1-d} \\
    &= 1 + \frac{m-1}{M-1} \cdot \frac{2d}{1-d}
    \end{align}
    
    Therefore, the multiplication factor for the $m$-th frame in terms of modulation depth $d$ is:
    
    \begin{equation}
        f_m = 1 + \frac{m-1}{M-1} \cdot \frac{2d}{1-d}
    \end{equation}
\end{comment}


\begin{figure}[H]
    \centering
    \includegraphics[width=0.93\textwidth]{figures/Figure_S9.pdf}
    \renewcommand{\thefigure}{S9}
    \captionof{figure}{\footnotesize\textbf{Effect of modulation depth on reconstruction quality.} The similarity score (y-axis) shows the Pearson correlation coefficient between the reconstructed and ground-truth images as modulation depth increases. Example reconstructions demonstrate the progressive elimination of background haze as modulation increases, while at higher modulation depths, some haze begins to appear within the object itself. The red circle indicates our chosen modulation depth, which offers a good trade-off in this balance.}
    \label{fig_S9}
\end{figure}
%\hlred{Explain  what is the similarity score in the caption, change y-label to correlation, write in the caption 'Pearson correlation'}

% \hlred{Add $\alpha$ to x label}


Our results show that without adding a modulation (modulation depth = 0), the reconstructions display a significant background haze. As the modulation depth increases, this background haze diminishes progressively. At very high modulation depths, while the background continues to reduce, the contrast is somewhat lowered, which may be the result of an imperfect estimation of the MTF in the I-CLASS algorithm \cite{weinberg2024noninvasive}.  Modulation depths of 0.3-0.5 used in our main experiments provide an effective trade-off between these competing effects. 
Additional improvements for addressing this point, such as SVD filtering, will be the focus of future work.

%\hlred{Maybe show the $PP^T$ for different $\alpha$-s}


\end{quote}
\end{ourresponse}


    
\begin{enumerate}[label=\arabic*., resume]
\item \leavevmode
\end{enumerate}
\vspace{-1em}
\begin{solved_reviewercomment}
    Even if the I-CLASS and CTR-CLASS have already been presented in previous papers, I would suggest describing them in the Methods section to make the paper self-sufficient. 
\end{solved_reviewercomment}

\begin{ourresponse}
We thank the reviewer for this important suggestion about making the paper self-contained, especially for the non-expert reader. We agree that providing a clear explanation of the I-CLASS algorithm is important for readers outside of the field to understand and potentially implement our approach.

To address this concern while balancing manuscript length considerations, we have added a comprehensive mathematical derivation and explanation of the matricial formalism of the problem of imaging through scattering media, and I-CLASS algorithm to the Supplementary Materials in a new section titled "Matrix-based scattering compensation algorithms." This detailed section:

\begin{enumerate}
    \item Establishes the mathematical foundations of conventional reflection-matrix imaging
    \item Shows how CTR-CLASS/I-CLASS extends this to random illumination scenarios
    \item Explains how our approach applies these principles to dynamic scattering through the mathematical duality between object and PSF
    \item Details the technical implementation of the memory-efficient algorithm
    \item \end{enumerate}

The mathematical nature and length of this explanation makes it more appropriate for the Supplementary Materials rather than the Methods section of the main paper. This approach follows common practice in the field where detailed algorithm derivations are included in supplementary sections to maintain focus on the experimental results and physical insights in the main text.
We kindly refer the reviewers to read the supplementary section in full in the Supplementary Materials section XX.

\end{ourresponse}


\begin{enumerate}[label=\arabic*., resume]
\item \leavevmode
\end{enumerate}
\vspace{-1em}
\begin{solved_reviewercomment}
    The same remark holds for the Fresnel operators applied at different stages of the post-processing for the holographic experiment.
\end{solved_reviewercomment}

\begin{ourresponse}
    We thank the reviewer for this suggestion. The revised manuscript now includes a detailed description of the Fresnel propagation operator used in the holographic reconstruction. This has been added to the \textbf{Methods} section titled "Fresnel propagation via Fourier-domain transfer function".
    \begin{quote}
        \subsection*{Fresnel propagation via Fourier-domain transfer function}
        \label{Fresnel_propagation}
        In Fig.~\ref{fig_5}, we present the object field located at the physical object plane, $z_{\text{obj}}$. However, since, following the principles of conjugate adaptive optics \cite{mertz2015field, kwon2023computational, sunray2024beyond, Katz2012looking}, we measure the fields at the diffuser plane and input these fields to the I-CLASS algorithm, the I-CLASS algorithm reconstructs the complex object field at the same plane. This is since the object field in our dynamic matrix approach is analogous to the scattering medium phase-function that CLASS/I-CLASS algorithms retrieve in the conventional static medium case. We denote this reconstructed object field at the scattering layer plane as  $E_o(x, y, z_{\text{scatt}})$. To visualize the field at the object plane, $E_o(x, y, z_{\text{obj}}) $, we back-propagate the reconstructed field from the diffuser plane to the object plane using Fresnel propagation under the paraxial approximation.
        
        This propagation is efficiently implemented in the Fourier domain using the Fresnel transfer function:
        
        \begin{equation}
            E_o(x, y, z_{\text{obj}}) = \mathcal{F}^{-1} \left\{ \tilde{E}_o(f_x, f_y, z_{\text{scatt}}) \cdot H(f_x, f_y; \Delta z) \right\}
        \end{equation}
        
        Where:
        - $ \tilde{E}_o(f_x, f_y, z_{\text{scatt}}) = \mathcal{F}\{ E_o(x, y, z_{\text{scatt}}) \} $ is the 2D Fourier transform of the reconstructed field,
        - $ H(f_x, f_y; \Delta z) $ is the Fresnel transfer function,
        - $ \Delta z = z_{\text{obj}} - z_{\text{scatt}} $ is the propagation distance,
        - $ \mathcal{F} $ and $ \mathcal{F}^{-1} $ denote the 2D Fourier and inverse Fourier transforms.
        
        The Fresnel transfer function in terms of spatial frequency is:
        
        \begin{equation}
            H(f_x, f_y; \Delta z) = \exp\left[ i \frac{2\pi \Delta z}{\lambda} \right] \cdot \exp\left[ -i \pi \lambda \Delta z (f_x^2 + f_y^2) \right]
        \end{equation}
        
        Here:
        - $ \lambda $ is the illumination wavelength,
        - $ (f_x, f_y) $ are the spatial frequency coordinates corresponding to the real-space axes $ (x, y) $.
        
        This formulation supports forward and backward propagation by simply changing the sign of $ \Delta z $, and is especially suitable for numerical implementation via Fast Fourier Transforms (FFT).
        
    \end{quote}

            
\end{ourresponse}


    
\begin{enumerate}[label=\arabic*., resume]
\item \leavevmode
\end{enumerate}
\vspace{-1em}
\begin{solved_reviewercomment}
    In that respect, it is not really clear to me why, in the holographic experiment, the I-CLASS reconstructed object field is obtained in the scattering layer plane? A more detailed description of the method is needed.
\end{solved_reviewercomment}

\begin{ourresponse}
    \begin{comment}
    \hlred{I would add that we measure on the diffuser plane and hence the multiplicative factor is with the propagated object, so that is the object that is reconstructed}


    \hlred{Write equations... original CLASS $POP\quad \overrightarrow{}\tilde{P}\tilde{O}\tilde{P}$, Here $O_{prop}PO_{prop}$ where $O_{prop}$ is diagonal and $P$ is the envelope, no Fourier needed}
    \end{comment}

    We thank the reviewer for bringing this potentially confusing point to our attention. 
    
    As mentioned in the new Methods section discussing the Fresnel propagator, since we measure the field at the scattering layer plane and input this field to the I-CLASS algorithm, the I-CLASS algorithm reconstructs the complex object field (equivalent in the conventional case to the scattering medium phase-function) also at the plane of the scattering layer. 
    The importance of measuring the field and applying CLASS/I-CLASS in the thin scattering layer plane follows the principle of conjugation in conjugate adaptive optics. Conjugate correction involves placing the corrective elements (or, in computational approaches, applying corrections) in a plane that is optically conjugated to the primary source of aberrations. By measuring and applying corrections at the plane where phase (and potentially also amplitude) multiplicative distortions occur (the thin scattering layer plane in our case), one achieves a more efficient isoplanatic correction across the field of view. This concept is well established in adaptive optics microscopy, such as in the works by Mertz et al. \cite{mertz2015field}, and others  \cite{kwon2023computational, sunray2024beyond}.
    
    In the standard case, the CLASS/I-CLASS algorithm finds the phase distortion at this exact plane where the distortions can be described as multiplicative distortions, i.e., the scattering layer transmission-matrix is a diagonal matrix that effectively describes a thin phase(and in I-CLASS also amplitude) mask.
    The output of the CLASS algorithm in the standard case, where the scattering medium is static and the target is dynamically illuminated, is the correction phase mask at the scattering layer plane. In our dynamic-scattering case, the scattering medium and the target change roles: the scattering function changes throughout the measurements while the object remains fixed. Running the I-CLASS algorithm on our dataset thus results in an output that is the object phase and amplitude function at the scattering layer plane. This complex-valued field is then back-propagated to any desired distance until a sharp, focused image of the target is obtained (see the new Supplementary Fig.~\ref{fig_S6}).

    To address this point in the revised manuscript, we have added the following explanation to the\textbf{ Methods} section, where we discuss the Fresnel reconstruction of the object field at the object plane:

    \begin{quote}
        "
        ...since, following the principles of conjugate adaptive optics \cite{mertz2015field, kwon2023computational, sunray2024beyond, Katz2012looking}, we measure the fields at the diffuser plane and input these fields to the I-CLASS algorithm, the I-CLASS algorithm reconstructs the complex object field at the same plane. This is since the object field in our dynamic matrix approach is analogous to the scattering medium phase-function that CLASS/I-CLASS algorithms retrieve in the conventional static medium case.
        "
    \end{quote}

\end{ourresponse}


\textbf{Typos:}

\begin{enumerate}[label=\arabic*., resume]
\item \leavevmode
\end{enumerate}
\vspace{-1em}
\begin{solved_reviewercomment}
    Page 2: “Thus, enabling a straightforward fully interpretable extension of matrix-based methods to rapidly dynamic scatterers, reconstructing complex megapixel-scale images through rapidly varying scattering. Importantly…” This is not a sentence, please rephrase.
\end{solved_reviewercomment}

\begin{ourresponse}
    We apologize for this phrasing mixup. We have revised the statement to read:
    
    \begin{quote}
        "Thus, our approach provides a natural and fully interpretable extension of matrix-based imaging techniques to the case of rapidly dynamic scatterers. 
        It enables the reconstruction of complex, megapixel-scale images through rapidly time-varying scattering. Importantly, unlike state-of-the-art neural-networks-based techniques, our approach does not require any assumptions on the temporal variations or other regularization, making it suitable for rapid dynamic scattering."
    \end{quote}
    
\end{ourresponse}



\begin{enumerate}[label=\arabic*., resume]
\item \leavevmode
\end{enumerate}
\vspace{-1em}
\begin{solved_reviewercomment}
    Page 7: “These experiments’ experimental setup…” Please rephrase.
\end{solved_reviewercomment}

\begin{ourresponse}
    We have revised the sentence to read: 
    \begin{quote}
        "The experimental setup and results for these experiments..." 
    \end{quote}
    which removes the redundancy and improves clarity.
\end{ourresponse}


\newpage

\section{Reviewer \#2 Comments (from email):}

In this manuscript the authors propose to use the I-CLASS algorithm (which was developed in the same group) to deconvolve the unknown object from the (many) unknown point spread function measured through a dynamic scattering medium. This builds on a previous paper from the same group (ref 14, as far as I can see still unpublished).

\textbf{Does the approach works?} Yes, the results in Fig. 2 alone are convincing enough.

\textbf{Are the result novel?} Yes. The algorithm itself is not new, but its application is.

\textbf{Is the paper written in a way such that somebody knowledgeable in optics but not an ultra-specialist in imaging through scattering media will be able to replicate the results and build up on them?} Not really. 

\begin{enumerate}[label=\arabic*.]
\item \leavevmode
\end{enumerate}
\vspace{-1em}
\begin{solved_reviewercomment}
    The main culprit is that the whole CRT-CLASS/I-CLASS are not well-known algorithms, and the only reference given to readers to make themselves familiar with them is not self-contained. \textbf{This has a very easy fix:} add a section in the supplementary information with an explanation of how and why the algorithm works (there is no space restriction there, so there is also no excuse to provide the necessary information).
\end{solved_reviewercomment}

\begin{ourresponse}
We thank the reviewer for raising this important point and for the constructive suggestion. Following the referee's proposal, we have added a comprehensive explanation of the I-CLASS algorithm to make the paper self-contained and accessible to readers who may not be specialists in imaging through scattering media, and to the potentially large readership that is not familiar with the recent advancement of matrix-based algorithms.
To this end, we have added a new supplementary section ("I-CLASS and matrix-based scattering compensation algorithms") that provides a complete mathematical foundation and derivation of:

\begin{enumerate}
    \item The reflection-matrix formulation of conventional imaging
    \item Reflection-matrix-based scattering compensation algorithm known as CLASS.
    \item The extension of the reflection-matrix formulation to random illuminations and the application of CTR-CLASS/I-CLASS
    \item The extension and application of I-CLASS to dynamic scattering through the exchange of roles between the object function and the scattering medium function
    \item The details of the memory-efficient implementation of I-CLASS algorithm for phase and amplitude correction.
\end{enumerate}

This new supplementary section reads:

\begin{quote}
    \subsection*{I-CLASS and matrix-based scattering compensation algorithms}

In this section, we provide the background and mathematical foundation for the I-CLASS algorithm used to reconstruct the target objects in our work. 
The I-CLASS algorithm builds upon the fundamental principles of reflection-matrix-based imaging techniques \cite{lee22}. To understand its operation, we first establish the mathematical foundation of the reflection-matrix formalism for imaging through static scattering media, and explain the basics of the original CLASS algorithm, applied to imaging with deterministic controlled illuminations, such as plane waves or scanning beams \cite{yoon2020laser}. 
We then proceed to show how the matricial formalism and the CLASS algorithm were extended to the case of random illuminations via CTR-CLASS \cite{lee22}, and the extension to phase and amplitude correction by I-CLASS \cite{weinberg2024noninvasive}. Finally, we explain how these principles apply to dynamic scattering by the exchange of roles between the scattering medium and the target object.

\subsubsection*{Reflection-matrix formalism}

We begin by considering a simple scenario of coherent imaging a reflective planar target object through a thin scattering layer. For simplicity, we assume isoplanatism, i.e., that the imaged field is a convolution of the field at the object plane by a complex-valued field point spread function (PSF). This is indeed the case for imaging objects that are smaller than the isoplanatic patch size\cite{bertolotti2022imaging, weinberg2024noninvasive}.
Under these conditions, the measured output field is given by:
\begin{equation}
E_{out}(\vec{r}) = P_{det}(\vec{r}) * E_{obj}(\vec{r})
\end{equation}
where $E_{obj}(\vec{r})$ is the reflected field at the object plane, and  $P_{det}(\vec{r})$ represent the detection PSF, which is the result of the combined effect of the scattering medium and the imaging system, and $*$ denotes convolution.

The field reflected from the object at the object plane, $E_{obj}(\vec{r})$, is the product of the object reflectivity $O(\vec{r}) $ and the illumination at the object plane. Since the considered scenario is of a target object that is hidden behind a thin scattering layer, the illumination at the target object plane is given by a convolution of the 'input' illumination field without the scattering medium present,$E_{in}(\vec{r})$, and the effective 'illumination PSF',  $P_{ill}(\vec{r})$, that results from the scattering medium and potentially also the illumination system. Thus, the measured output field behind the scattering layer when an input illumination $E_{in}(\vec{r})$ is used is given by:

\begin{equation}
E_{out}(\vec{r}) = P_{det}(\vec{r}) * [O(\vec{r}) \cdot (P_{ill}(\vec{r}) * E_{in}(\vec{r}))]
\label{eq: fund. imaging}
\end{equation}

%where $P_{ill}(\vec{r})$ and $P_{det}(\vec{r})$ represent the illumination and detection point spread functions (PSFs), respectively, $O(\vec{r})$ is the object reflectivity function, and $*$ denotes convolution.

%When the output field is measured for multiple different input fields $E_{in,m}(\vec{r})$ for $m=1...M$, i obtain a set of output fields:

%\begin{equation}
%E_{out,m}(\vec{r}) = P_{det}(\vec{r}) * [O(\vec{r}) \cdot (P_{ill}(\vec{r}) * E_{in,m}(\vec{r}))]
%\end{equation}

When sampling these fields and PSFs on a discrete grid with $N$ pixels, and arranging the fields into column vectors with $N$ entries each, the linear relationship given by Supp.Eq. ~\ref{eq: fund. imaging} above can be expressed in matricial form as:
\begin{equation}
\vec{E}_{out} = \mathbf{P}_{det} \mathbf{O} \mathbf{P}_{ill} \vec{E}_{in} \equiv \mathbf{R} \vec{E}_{in} 
\end{equation}


Here, $\mathbf{P}_{ill}$ and $\mathbf{P}_{det}$ are $N \times N$ Toeplitz (convolution) matrices representing the illumination and detection PSFs (with the shifted PSFs as their columns), and $\mathbf{O}$ is a diagonal matrix with the object complex-valued reflectivity on its diagonal. The product of the illumination matrix, object matrix, and detection matrix describes the propagation of any input field through the scattering medium to the object and back from the object through the scattering medium to the detection system. This matrix product forms the reflection matrix of the complex medium and the target object, which has a characteristic Toeplitz-Diagonal-Toeplitz (TDT) structure:
\begin{equation}
    \mathbf{R} \equiv \mathbf{P}_{det} \mathbf{O} \mathbf{P}_{ill}
    \label{eq: ref.matrix definition}
\end{equation}


For a static scattering medium and object scenario, one can measure the reflection matrix column by column by illuminating the medium with $m=1..N$ controlled input fields (basis vectors,) and recording the output scattered light fields for each of these $m=1..N$ modes: $\vec{E}_{out,m} =  \mathbf{R} \vec{E}_{in,m} $.

Once the reflection matrix is measured, the challenge of reconstructing the image of the hidden object from the scattered light measurements is equivalent to decomposing the reflection matrix into its three components. 
The CLASS algorithm \cite{choi2022flexible} is fundamentally a matrix decomposition method that can take any matrix with this TDT structure and decompose it into its constituent matrices. This allows recovery of the aberration-free object reflectivity function, $\mathbf{O}$, by separating it from the distortions introduced by the PSFs. 
Importantly, such a decomposition is made possible due to the fact that while the full reflection matrix contains $N\times N = N^2$ elements (measurements), it is the product of three matrices where in each matrix there are only $N$ unknown elements. Thus the challenge is to retrieve the $3N$ unknowns (target object and two PSFs) from $N^2$ measurements.

\subsubsection*{The CLASS algorithm}
\textbf{[Here there should be an explanation of how the CLASS algorithm works - how it finds the unknowns from the correlations between the reflection matrix columns - take from Yevgeny]}
 
\subsubsection*{CTR-CLASS and I-CLASS: retrieving a reflection matrix from measurements using unknown input fields}
In the case that the input fields for measuring the reflection matrix cannot be controlled or are unknown, one can still measure the resulting scattered light field and retrieve a 'virtual reflection matrix' from the covariance matrix of these fields. This concept for 'compressive time-reversed' (CTR) measurement of the reflection matrix was introduced by Lee et al. \cite{lee22}, and forms the basis for what is termed a CTR-CLASS approach.
In CTR-CLASS, instead of acquiring the full matrix by $N$ measurements, one illuminate the object with $M\leq N$ different random and unknown fields, $S_{m}(\vec{r})=P_{ill}*E_{in,m}(\vec{r})$. For each illumination, the field equation becomes:

\begin{equation}
E_{out,m}(\vec{r}) =  P_{det}(\vec{r}) *[O(\vec{r}) S_{m}(\vec{r})] = P_{det}(\vec{r}) * O_m(\vec{r})
\end{equation}

where $O_m(\vec{r}) = O(\vec{r}) S_{m}(\vec{r})$ represents the m-th reflected field from the object.

Arranging these $M$ measurements as columns of a 'measuremebnt matrix', $\mathbf{A}$, we have:

\begin{equation}
\mathbf{A} = \mathbf{P}_{det} \mathbf{O} \mathbf{S}
\end{equation}

where $\mathbf{S} $ is a matrix that contains the illumination patterns at the object plane,$S_{m}(\vec{r})$, as its columns. 

Importantly, if the illumination patterns are random and uncorrelated, the covariance matrix of $\mathbf{A}$ takes the form:

\begin{equation}
\mathbf{A} \mathbf{A}^{\dagger} = \mathbf{P}_{det} \mathbf{O} (\mathbf{S} \mathbf{S}^{\dagger}) \mathbf{O}^{\dagger} \mathbf{P}_{det}^{\dagger} \approx \mathbf{P}_{det} |O|^2 \mathbf{P}_{det}^{\dagger}
\label{eq: cov form}
\end{equation}

where we have used the fact that  $\mathbf{S} \mathbf{S}^{\dagger} \approx \mathbf{I}$ for uncorrelated illuminations. 

The result of Supp.Eq.~\ref{eq: cov form} shows that the covariance matrix of scattered light fields obtained under random unknown illumination has the same TDT structure as the conventional reflection matrix (Supp. Eq.~\ref{eq: ref.matrix definition}) with $|O|^2$ replacing $O$. Thus, the CLASS algorithm can be applied directly to this covariance matrix to retrieve both the absolute value of the object reflectivity from $|O|^2$, and the detection PSF, $P_{det}$.

While the CLASS algorithm was developed to compensate for phase-only distortions, i.e. to the case where the illumination or detection PSF are each given by a Fourier transform of a phase-only mask, the recently introduced I-CLASS algorithm \cite{weinberg2024noninvasive} extends the CLASS correction (or equivalently, matrix decomposition) to the case where both amplitude and phase distortions exists. 

\subsubsection*{Dynamic scattering: object and PSF role exchange}
==================  I AM HERE  ========================
In our dynamic scattering scenario, the fundamental imaging equation changes. Instead of having a static PSF and varying illuminations, we have a static object and varying PSFs due to the dynamic scattering medium, the imaging equation in this case will be:

\begin{equation}
I_m(\vec{r}) = P_m(\vec{r}) * O(\vec{r}) = O(\vec{r}) * P_m(\vec{r})
\end{equation}

Where $P_m$ is the PSF for the $m$-th realization of the dynamic scatterer. The crucial insight is that due to the commutativity of convolution, this equation has exactly the same mathematical form as the random illumination case described above, but with the roles of the object and PSF interchanged.

(For the coherent imaging case, we can safely ignore variations in the illumination PSF $P_{ill}$ by focusing the illumination beam to a spot size smaller than the correlation length of the diffuser. This ensures that the illumination samples only a single "phase patch" of the diffuser, resulting in a relatively constant illumination pattern at the object plane across different diffuser realizations (see Supplementary Section on "Effect of illumination spot size").

For the incoherent imaging case, the rapid rotation of the diffuser in the illumination path creates many speckle patterns within each camera exposure, effectively resulting in spatially uniform illumination. This allows us to focus solely on the variations in the detection PSF.)

Due to this mathematical equivalence between the dynamic scattering and random illumination scenarios, we can apply the same CTR-CLASS/I-CLASS algorithm to reconstruct the object, with only a reinterpretation of the results needed.

\subsubsection*{I-CLASS implementation}

The I-CLASS algorithm extends the CLASS approach with two key improvements:
\begin{enumerate}
    \item a memory-efficient implementation that avoids explicitly computing the full $N \times N$ covariance matrix
    \item correction for both phase and amplitude distortions.
\end{enumerate}
For each iteration $t$ of the algorithm, the phase correction is calculated using:

\begin{equation}
\vec{z}_{t+1} = \sum_{q=1}^{M} (\tilde{\mathbf{A}}_t^* \odot ((\tilde{\mathbf{A}}_t^{(ud)*} * \tilde{\mathbf{A}}_t^{(lr)})_{:,M-1} \star \tilde{\mathbf{A}}_t^{(ud)}))_{:,q}
\end{equation}

where $\tilde{\mathbf{A}}$ is the 2D Fourier transform of $\hat{\mathbf{A}}$, $\tilde{\mathbf{A}}^{(ud)}$ is $\tilde{\mathbf{A}}$ with each column flipped upside-down, $\tilde{\mathbf{A}}^{(lr)}$ is $\tilde{\mathbf{A}}$ with its rows flipped left-to-right, $\odot$ denotes the Hadamard product, $*$ denotes 2D convolution, and $\star$ denotes 2D correlation.

The phase correction is then applied as:

\begin{equation}
\tilde{\mathbf{A}}_{t+1} = diag\{e^{i\frac{\vec{z}_t}{|\vec{z}_t|}}\} \tilde{\mathbf{A}}_t
\end{equation}

Where the exponential and division operations are element-wise.

Beyond phase correction, I-CLASS also estimates the amplitude distortions in the Fourier domain (the Modulation Transfer Function, MTF) from the diagonal of the covariance matrix in the Fourier domain. This amplitude information is used to correct for the amplitude aberrations.

In the conventional I-CLASS application, after applying both phase and amplitude corrections, the algorithm produces the PSF's phase and amplitude, and $|O|^2$ as the components of the covariance matrix. However, in our case with the "role exchange" between object and PSF, the algorithm instead produces the phase (in the coherent case) and amplitude of the reconstructed object, along with $|PSF|^2$. This role reversal is the key insight that allows us to apply the same algorithm to dynamic scattering media.
    
\end{quote}

We structured this section to gradually introduce the concepts, starting from the basic principles of reflection matrices and progressing to our specific implementation for dynamic scattering.

We believe these additions significantly improve the paper's accessibility and provide readers with all necessary background to understand and implement our method.

\end{ourresponse}


\begin{enumerate}[label=\arabic*., resume]
\item \leavevmode
\end{enumerate}
\vspace{-1em}
\begin{solved_reviewercomment}
    Another point that is likely to make the life on a non-specialist unnecessarily hard is that eq. 1 is only valid within the isoplanatic patch. For objects larger than the isoplanatic patch nothing of what is presented here will work. This is only briefly addressed at the end of the Discussion section (largely swiping it under the carpet) but if not tackled at the beginning is likely to confuse people.
    
\end{solved_reviewercomment}

\begin{ourresponse}
    We appreciate the reviewer's concern about the isoplanatic patch limitation. We had already mentioned this constraint in the opening paragraph of the Principle section, highlighting that "all experiments and analysis in this work require the imaged object to be contained within the isoplanatic patch of the scattering medium; for objects larger than this patch, the approach presented here would not directly apply without additional modifications such as field segmentation."

    To further emphasize this point and ensure clarity for non-specialist readers, we have also added an explicit note immediately following Equation 1 reiterating this constraint:
    
    \begin{quote}
        "It is important to note that this convolution model is strictly valid only for objects within an isoplanatic patch. All experiments in this work were designed within this constraint, with potential extensions to larger fields of view or thick complex media, which are discussed in the final section."
    \end{quote}
    
    This addition ensures that readers are aware of this limitation from the outset, helping non-specialists better understand the applicability of our approach without having to wait until the Discussion section.
\end{ourresponse}



A few more minor points:

\begin{enumerate}[label=\arabic*., resume]
\item \leavevmode
\end{enumerate}
\vspace{-1em}
\begin{solved_reviewercomment}
    In the introduction the authors claim that iterative phase retrieval lacks guaranteed convergence, which is technically not true. Convergence might take a VERY long time, but it will eventually happen (see \url{https://doi.org/10.1364/AO.21.002758}).
\end{solved_reviewercomment}
\begin{comment}
    ~\\
   \hlred{I recommend applying phase retrieval for millions of iterations with large M to show convergence graph}             
 \end{comment}

\begin{ourresponse}
    We thank the reviewer for their insight. We have revised the text to more accurately reflect the nature of iterative phase retrieval, acknowledging that convergence is possible, albeit potentially time-consuming. The revised sentence now reads:  
    \begin{quote}
        "However, despite this advantage, these techniques are hindered by their reliance on iterative phase retrieval \cite{fienup1978reconstruction}, which can require extensive computational time to converge."
    \end{quote}
    
\end{ourresponse}



\begin{enumerate}[label=\arabic*., resume]
\item \leavevmode
\end{enumerate}
\vspace{-1em}
\begin{solved_reviewercomment}
    Just below the above statement there are two sentences beginning with "while" which looks like they are the leftover of some copy-paste during editing.
\end{solved_reviewercomment}
\begin{ourresponse}
    We thank the reviewer for their careful reading. We have removed the redundant sentence and simplified the text to improve clarity and flow. The revised text now reads:
    \begin{quote}
        "While deterministic bispectrum reconstruction can address the convergence challenge of phase retrieval, it still requires averaging a large number of speckle grains, limiting the reconstruction to simple objects."
    \end{quote}
\end{ourresponse}
% \hlred{Write something about that it is not really cheating since in incoherent illumination passive constant illumination such as the sun...}

        
\begin{enumerate}[label=\arabic*., resume]
\item \leavevmode
\end{enumerate}
\vspace{-1em}
\begin{solved_reviewercomment}
    The results shown in Fig. 2 and 3, albeit impressive, require the target to be illuminated from behind, which makes this whole approach invasive. I know the author never explicitly claim non-invasiveness, but they also never make this point clear.
\end{solved_reviewercomment}

\begin{ourresponse}
    We thank the reviewer for highlighting this important point.
    We fully agree that the initial results presented in Figures 2 and 3 require illumination from behind the sample, which makes the approach invasive. 
    
    However, it is worth noting that this illumination geometry is not fundamentally different from standard practices in incoherent imaging. For instance, passive illumination sources like sunlight routinely provide constant, uniform illumination from behind a sample – a technique widely accepted in microscopy and imaging sciences.
    
    To clarify this explicitly in the revised manuscript, we added the following note:
    \begin{quote}
        "\textbf{Note:} The matrix-based imaging approach demonstrated here highlights the fundamental principle of imaging through dynamic scattering media. Although these initial experiments utilize a transmission geometry with illumination from behind the sample, our subsequent experimental demonstrations in fluorescence microscopy (Fig.4) and coherent holographic imaging Fig.~\ref{fig_5}) showcase the technique's adaptability across diverse optical configurations and imaging modalities."
    \end{quote}
    
    Thus, although the particular results in Figures 2 and 3 are invasive, the manuscript demonstrates a non-invasive adaptation of our method (as shown in Figures 4 and 5).
\end{ourresponse}



\begin{enumerate}[label=\arabic*., resume]
\item \leavevmode
\end{enumerate}
\vspace{-1em}
\begin{solved_reviewercomment}
I might have missed it, but I don't think I have seen any discussion about the time needed for the I-CLASS algorithm to converge and give the claimed results.
\end{solved_reviewercomment}
\begin{ourresponse}
Thank you for the opportunity to clarify the computational performance of our algorithm. To give readers a practical sense of the method's efficiency, we've added a runtime specification in the Experimental Parameters section:

\begin{quote}
    The algorithm run time on a commercially available GPU (Nvidia RTX4090, 24 GB) was approximately $\sim 8ms$ per iteration for 150 camera frames at a resolution of $300 \times 300$ pixels and around $\sim 70ms$ per iteration for 150 camera frames at a resolution of $850 \times 850$ pixels.
\end{quote}
\end{ourresponse}



        
\begin{enumerate}[label=\arabic*., resume]
\item \leavevmode
\end{enumerate}
\vspace{-1em}
\begin{solved_reviewercomment}
    For the results shown in Fig.~\ref{fig_5} one needs to know the distance between the object and the scattering medium, as only the field at the scattering layer can be reconstructed, and one needs to know how far to propagate it back.
\end{solved_reviewercomment}

\begin{ourresponse}
    We thank the reviewer for this insightful question regarding the determination of the correct propagation distance. While the I-CLASS algorithm initially reconstructs the complex field at the scattering layer plane, a key advantage of our coherent imaging approach is that once we have retrieved the complex wavefront, we can numerically propagate it to any desired axial plane.
    
    This "digital autofocus" capability is a fundamental strength of coherent computational imaging. As we now explain in detail in the Methods section titled "Fresnel Propagation via Fourier-Domain Transfer Function," we use the Fresnel propagation transfer function to computationally propagate the reconstructed field from the diffuser plane to any desired plane, including the object plane.
    
    This numerical propagation capability allows us to determine the correct object plane computationally by finding the propagation distance at which the target features achieve optimal focus—eliminating the need for precise a priori knowledge of the object-diffuser distance during data acquisition. This is analogous to the focusing capability in digital holographic microscopy but applied to imaging through scattering media.
    
    To comprehensively address this point, we have added Supplementary Movie S3, which shows the continuous evolution of the reconstructed field as it is propagated from the diffuser plane through the optimal focus plane and beyond. We have also created a new supplementary section titled "Digital Autofocus through Fresnel Propagation" that demonstrates this capability in detail, which we reproduce below for the reviewer's convenience:

    \begin{quote}
        \section*{Digital auto-focus through Fresnel propagation}
        
        In holographic imaging through scattering media, a key advantage is the ability to numerically propagate the reconstructed complex field to any desired axial plane once the field is retrieved at a single reference plane. This capability, often called digital autofocus, eliminates the necessity of knowing the exact object distance during data acquisition.
        
        In our experiments from Fig.~\ref{fig_5} of the main text, the I-CLASS algorithm reconstructs the complex wavefront at the scattering layer plane (z = 0). However, as shown in Fig.~\ref{fig_S6}, this field can be digitally propagated to any desired plane using the Fresnel propagation operator described in the Methods section of the main text. 
        
        By computationally varying the propagation distance and observing the resulting reconstructed intensity distributions, we can identify the optimal object plane where the finest features of the target come into focus. This process is analogous to the physical process of adjusting the focus in a conventional microscope, but performed entirely in post-processing.
        
        Figure~S6 demonstrates this capability by showing the reconstructed field intensity at three distinct propagation distances: before the object plane (z = 5.3 cm), at the object plane where optimal focus is achieved (z = 7.15 cm), and after the object plane (z = 9 cm). The sharp focus observed at z = 7.15 cm confirms that this is indeed the correct object plane, with clear resolution of the fine features in the USAF target.
        
        For a more comprehensive demonstration of this digital autofocus capability, we refer readers to Supplementary Movie S3, which shows the continuous evolution of the reconstructed field as the propagation distance is varied from the diffuser plane (z = 0) to beyond the object plane.

        \vspace{1em}
        \begin{figure}[H]
            \centering
            \includegraphics[width=0.93\textwidth]{figures/figure_S6.pdf}
            \renewcommand{\thefigure}{S6}
            \captionof{figure}{\footnotesize\textbf{Digital autofocus capability through numerical propagation of the reconstructed complex field.} The reconstructed field intensity is shown at three different propagation distances: \textbf{a} z = 5.3 cm (before the object plane), \textbf{b} z = 7.15 cm (at the object plane, where optimal focus is achieved), and \textbf{c} z = 9.0 cm (after the object plane). The sharp focus visible in panel \textbf{b} confirms the correct object plane location, demonstrating that precise knowledge of the object-diffuser distance is not required during data acquisition as optimal focus can be determined computationally during post-processing. Scale bars: 1 mm.}
            \label{fig_S6}
        \end{figure}
    \end{quote}
\end{ourresponse}


        
\begin{enumerate}[label=\arabic*., resume]
\item \leavevmode
\end{enumerate}
\vspace{-1em}
\begin{solved_reviewercomment}
    I am not sure I understand how focussing the illumination on the scattering medium can reduce the fluctuations of \(E^{\text{ill}}_m\). Wouldn't it actually maximise them?    
\end{solved_reviewercomment}
\begin{ourresponse}
We thank the reviewer for this insightful question about an important aspect of our experimental design.

The reviewer's intuition that focusing on a scattering medium might maximize illumination fluctuations is valid for many scenarios. However, in our specific configuration, the opposite effect occurs due to a crucial relationship between the illumination spot size and the correlation length of the diffuser.

To address this question comprehensively, we have added a new section ("Effect of illumination spot size on coherent imaging through dynamic scattering") to the supplementary material with numerical simulations demonstrating this effect. For completeness, we reproduce the full section below:
\begin{quote}
    \section*{Effect of illumination spot size on coherent imaging through dynamic scattering}

In our coherent imaging configuration (Fig.~\ref{fig_5} of the main text), maintaining a constant illumination pattern at the object plane despite the dynamic scattering introduced by the rotating diffuser is crucial for successful application of our reconstruction algorithm. As expressed in Eq. 6-7 of the main text, our method requires that the illumination field $E_m^{ill}(\vec{r})$ remains effectively constant across different acquisitions for our method to work properly.

This requirement might appear contradictory, as one might expect that any illumination passing through a dynamic scatterer would inevitably result in varying illumination patterns. However, the key insight is that the relationship between the illumination spot size on the diffuser and the diffuser's correlation length determines whether the illumination at the object plane remains approximately constant between different diffuser realizations.


\begin{figure}[H]
\centering
\includegraphics[width=0.99\textwidth]{figures/figure_S7.pdf}
\renewcommand{\thefigure}{S7}
\captionof{figure}{\footnotesize\textbf{Effect of illumination spot size on imaging through dynamic scattering.} 
Numerical simulations demonstrating how the ratio of illumination spot size ($d_{spot}$) to diffuser correlation length ($d_{corr}$) affects illumination at the object plane and reconstruction quality. Each column represents a different spot size ratio: $\frac{d_{spot}}{d_{corr}}=0.1$ (left), $\frac{d_{spot}}{d_{corr}}=1$ (middle), and $\frac{d_{spot}}{d_{corr}}=10$ (right). \textbf{a-c} Illumination phase pattern on the diffuser surface. The colorbar indicates phase values from $-\pi$ to $\pi$. The white inset in (a) shows the zoomed-in beam spot. \textbf{d-f} Resulting intensity patterns at the object plane after numerical propagation through the diffuser. The grayscale colorbar indicates normalized intensity. \textbf{g-i} Effective object (target object multiplied by the illumination pattern). \textbf{j-l} Reconstructed images after applying the I-CLASS algorithm. Note how a small spot size relative to the diffuser correlation length (left column) maintains relatively uniform illumination at the object plane, enabling high-quality reconstruction, while larger spot sizes (middle and right columns) create varying speckle patterns that degrade reconstruction quality}
\label{fig_S7}
\end{figure}

Fig.~\ref{fig_S7} demonstrates this principle through numerical simulation. The top row (Fig.~\ref{fig_S7}a-c) shows the phase pattern of the diffuser with the illumination spot superimposed for three cases: where the spot size is 0.1, 1, and 10 times the diffuser correlation length ($d_{\text{correlation}}$), respectively. The second row (Fig.~\ref{fig_S7}d-f) shows the resulting illumination intensity pattern at the object plane after the light has propagated through the diffuser, simulated using Fourier transform propagation.

When the illumination spot is much smaller than the diffuser correlation length (Fig.~\ref{fig_S7}a), it effectively samples only a single "phase patch" of the diffuser. As the diffuser rotates between realizations, this results in a relatively uniform illumination at the object plane that experiences only a global phase shift but maintains its spatial intensity pattern across different realizations. This consistency across different diffuser positions is crucial for our method to work properly. In other words, while the global phase of the illumination might change, its spatial distribution pattern remains effectively identical from one diffuser position to the next, satisfying our requirement for constant spatial illumination.

In contrast, when the illumination spot size is comparable to or larger than the diffuser correlation length (Fig.~\ref{fig_S7}c), the beam simultaneously samples multiple uncorrelated regions of the diffuser. This produces complex speckle patterns at the object plane that vary significantly between diffuser positions, creating a different illumination pattern for each diffuser realization. This variation violates our assumption of constant illumination in Eq. 6-7 of the main text.

The third row (Fig.~\ref{fig_S7}g-i) shows the effect of these illumination patterns on the effective object (the product of the object and the illumination), while the fourth row (Fig.~\ref{fig_S7}j-l) shows the reconstruction results. The reconstruction quality clearly degrades as the spot size increases relative to the diffuser correlation length, demonstrating the critical importance of maintaining consistent illumination across different diffuser realizations.

In our experimental implementation described in the main text, we carefully focused the beam to ensure a spot size smaller than the diffuser's correlation length ($\approx 70 \mu m$), thereby maintaining nearly constant spatial illumination patterns at the object plane across different realizations, with variations limited primarily to global phase shifts.
\end{quote}

\end{ourresponse}

\begin{enumerate}[label=\arabic*., resume]
\item \leavevmode
\end{enumerate}
\vspace{-1em}
\begin{solved_reviewercomment}
    I am slightly confused by the setup diagram in Fig.~\ref{fig_5}: The role of the polarizing beam splitter near the scattering medium is clear, as it allows to reject part of the unscattered light, thus maximizing the amount of useful signal, but the polarizing beam splitter closer to the camera seems to ensure that the signal and the reference have opposite polarizations, and thus can never interfere. Am I missing something, or is the diagram wrong?
\end{solved_reviewercomment}

\begin{ourresponse}
    We thank the reviewer for their careful examination of our experimental setup. The reviewer is correct - this was an error in our diagram. It should indeed be a non-polarizing beam splitter (BS), not a polarizing beam splitter (PBS). We have corrected this in the revised manuscript.
            
    \vspace{1em}
    \begin{figure}[H]
        \centering
        \includegraphics[width=0.93\linewidth]{figures/figure_5.pdf}
        \renewcommand{\thefigure}{5}
        \captionof{figure}{\footnotesize\textbf{Experimental coherent reflection-imaging through dynamic scattering.} 
        \textbf{a} Experimental setup: A reflective target is illuminated through a dynamically rotating scattering diffuser. $M=180$ reflected light fields are holographically recorded in an off-axis holography configuration using a reference arm. %The illumination beam is focused to a tight spot on the diffuser surface such that the object illumination remains relatively constant while the diffuser is rotated.
        \textbf{b} Example of the recorded distorted fields after computational propagation to the object plane. \textbf{c} One example of the recorded field intensity after computational propagation to the object plane. 
        \textbf{d} Reconstructed object intensity at the object plane after applying the I-CLASS algorithm to compensate for scattering, revealing the details of the target. 
        \textbf{e} Complex-valued field amplitude PSFs (APSFs), estimated from the captured fields after effective deconvolution of the reconstructed object field. 
        \textbf{f} Reference intensity image of the object without the diffuser present. Scale bars, 1$mm$}
        \label{fig_5}
    \end{figure}
\end{ourresponse}
        
\begin{enumerate}[label=\arabic*., resume]
\item \leavevmode
\end{enumerate}
\vspace{-1em}
\begin{solved_reviewercomment}
    The comparison between I-CLASS and the phase retrieval algorithms in the supplementary information seems weird. The fact that the phase retrieval is shown to never be able to reconstruct any image more complex than a few dots looks too bad to be true, and is at odds with my personal experience.
\end{solved_reviewercomment}

\begin{ourresponse}
    We thank the reviewer for raising this concern about the comparison between I-CLASS and phase retrieval-based methods.
    
    The comparison shown in the supplementary information is between I-CLASS and speckle correlation imaging that uses phase retrieval on an \textbf{estimated} autocorrelation, not standard phase retrieval on the true object spectrum.
    
    Standard phase retrieval operates directly on the exact power spectrum of an object, whereas speckle correlation imaging first requires estimating the object's autocorrelation from multiple scattered light patterns. The quality of this estimation affects the subsequent phase retrieval results.
    
    Following the reviewer's comment, we have revisited our speckle correlation implementations and improved the results by applying a more rigorous approach:
    
    For autocorrelation estimation:
    \begin{enumerate}
        \item Calculate the autocorrelation of each scattered frame
        \item Average all autocorrelations
        \item Divide the result by the autocorrelation of the mean of the images, which provides an estimate of the envelope of the PSF and subtract the envelope from all frames
        \item Subtract the minimum value to ensure non-negativity
        \item Apply appropriate windowing (square root of Tukey window for USAF target and fourth root of Tukey window for the cameraman)
    \end{enumerate}
    
    For phase retrieval, following Katz et al. (Nature Photonics, 2014):
    \begin{enumerate}
        \item Apply 300 iterations of HIO algorithm with decreasing beta parameters (from 2.0 to 1.2, decreasing by 0.2)
        \item Follow with 300 iterations of error reduction
        \item Implement object-specific constraints:
        \begin{itemize}
            \item  For the cameraman: real, positive, and non-negative constraints
            \item  For the USAF target: same constraints plus an additional finite square support
        \end{itemize}
    \end{enumerate}
    
    
    The updated figure now includes both our improved speckle correlation results and phase retrieval applied directly to the reference image for comparison. While our enhanced speckle correlation implementation shows better results than our original submission, I-CLASS still demonstrates better reconstruction quality, particularly for complex objects when the number of speckles in the PSF is limited, as is often the case in realistic experimental conditions.
    
    We have updated the supplementary section to reflect these improvements and provide a more comprehensive comparison.

                \begin{quote}
        \section*{Comparison with speckle-correlation phase-retrieval based reconstruction}

        We present a numerical comparison between our proposed I-CLASS method and the speckle correlation imaging of Katz et al. \cite{katz14}, which is, in essence, equivalent to running a phase-retrieval algorithm on the \textbf{estimated} object autocorrelation (that is, its power spectrum) as originally proposed by Labeyrie's stellar speckle interferometry \cite{labeyrie1970attainment}. 
        Both methods utilize similar experimental setups and image acquisition schemes, allowing for a direct performance evaluation. To perform this comparison, we focused on several numerically simulated isoplanatic imaging scenarios, demonstrating the superior performance of the matricial I-CLASS approach when reconstructing complex non-sparse natural target objects.
        
        For the speckle correlation reconstruction, we employ a rigorous approach to estimate the autocorrelation:
        \begin{enumerate}
            \item Calculate the autocorrelation of each scattered frame
            \item Average all autocorrelations
            \item Divide the result by the autocorrelation of the mean of the images, which provides an estimate of the envelope of the PSF and subtract the envelope from all frames
            \item Subtract the minimum value to ensure non-negativity
            \item Apply appropriate windowing (sqrt of Tukey window for USAF target and Tukey window $^{(1/4)}$ for the cameraman)
        \end{enumerate}
        
        For phase retrieval, following Katz et al. \cite{katz14}, we implement:
        \begin{enumerate}
            \item 300 iterations of HIO algorithm with decreasing beta parameters (from 2.0 to 1.2, decreasing by 0.2)
            \item 300 additional iterations of error reduction
            \item Object-specific constraints:
            \begin{itemize}
                \item For the cameraman: real, positive, and non-negative constraints
                \item For the USAF target: same constraints plus an additional finite square support
            \end{itemize}
        \end{enumerate}
        
        The results of this comparison are presented in Fig.~\ref{fig_S4}. The top row shows results for the Cameraman image, while the bottom row shows results for the USAF resolution target. For each test object, we display the widefield reference (Fig.~\ref{fig_S4}a,f), an example frame distorted by scattering (Fig.~\ref{fig_S4}b,g), the speckle correlation reconstruction (Fig.~\ref{fig_S4}c,h), the I-CLASS reconstruction (Fig.~\ref{fig_S4}d,i), and phase retrieval applied directly to the reference image (Fig.~\ref{fig_S4}e,j).
        
        While speckle correlation imaging produces recognizable reconstructions for both test objects, I-CLASS demonstrates higher reconstruction quality, particularly for the more complex Cameraman image.

        \begin{figure}[H]
            \centering
            \includegraphics[width=0.93\linewidth]{figures/figure_S4.pdf}
            \renewcommand{\thefigure}{S4}
            \captionof{figure}{\footnotesize\textbf{Numerical example imaging through dynamic scattering media.} 
            A comparison between I-CLASS and speckle correlation imaging for complex objects. \textbf{a,f} Numerically simulated target objects: the Cameraman image and USAF resolution target. \textbf{b,g} Example camera frames simulated under dynamic scattering. \textbf{c,h} Speckle correlation imaging reconstructions using the HIO algorithm (see parameters in text). \textbf{d,i} I-CLASS reconstructions showing higher fidelity to the original objects. \textbf{e,j} Phase retrieval applied directly to the reference images. Scale bar: 10 px.}
            \label{fig_S4}
        \end{figure}
    \end{quote}

\end{ourresponse}

\begin{comment}
\hlyellow{\textbf{Note from Ori:}
the reason is that this is not standard phase-retrieval but phase-retrieval of speckle correlation imaging experiment: in a standard phase-retrieval the algorithm is run on the real power spectrum on the object (or equivalently the exact fourier-transform autocorrelation of the object) calculated directly from the object, whereas in our case the phase retrieval is run on an ESTIMATE of the object’s autocorrelation from the speckle correlation. In particular, for speckle correlation imaging to work, the speckled PSF autocorrelation has to resemble a highly-peaked delta-like function. This is the case ONLY when the PSF contains an extremely large number of speckles, since for a speckle field the autocorrelation peak to background ratio is proportional to the number of speckles contained in the PSF. This practically requires the captured scattered light image to contain a number of speckles that is considerably larger than the number of bright resolution cells in the target, which is not the case in our experiemnts and simulations]}
\hlred{Here we need to answer that it is correct that if you have the correct PSD of the object given some constraint there is a global minimum and you probably eventually converge there, however given non ideal estimation of the PSD (statistical speckle statistics error + noise) it works much worse, regrading statistical noise I would discuss the small PSF regime, and about the shot noise, I would cite Jerome's Optica paper about covariance noise, and claim that since we use the entire matrix and not just the diagonal we have better SNR}
\end{comment}

            
        \hl{example 1}
        
        \hlblue{example 2}
        
        \hlgreen{example 3}
        
        \hlred{example 4}
        
        \hlyellow{example 5}
        


\begin{comment}
    \textbf{This is the part copied from, and has to be added to the main manuscript:}
        \begin{quote}
            \subsection{CTR-CLASS}
            %Several key techniques for imaging through scattering layers within the 'memory-effect' region involve measuring the imaging system's transmission matrix. Recent advancements \cite{lee22,weinberg2024noninvasive} propose that, as the distortion caused by illumination does not hold any informative value, it's not necessary to measure the entire large matrix. Instead, one can effectively use a limited number ($M$) of random, uncorrelated illuminations for imaging, rather than analyzing the full matrix.
            Following the work of Lee et al. \cite{lee22}, we consider $m=1...M$ measured scattered light fields, $\vec{A}_m$ obtained under random uncorrelated illuminations, $\vec{S}_m$. By representing the optical transmission matrix from the illumination plane to the target plane as $\bm{P}_{ill}$, and the transmission matrix from the target plane to the camera plane by $\bm{P}_{det}$, each measured light field can be written as $\vec{A}_m = \bm{P}_{det}\bm{O} \bm{P}_{ill} \vec{S}_m$. The matrix $\bm{O}$ is the object transmission matrix. For a thin object, the matrix $\bm{O}$ is a diagonal matrix with the object transmission function along its diagonal.
            By incorporating the $M$ measurements, $\vec{A}_m$, as columns in a matrix $\bm{A}$, the entire measured dataset can be represented in a matrix form as:
            \begin{eqnarray}
            \bm{A} = \bm{P}_{det}\bm{O} \bm{P}_{ill} \bm{S}
            \label{eq:1}
            \end{eqnarray}
            where $\bm{S}$ is a matrix containing the input random patterns in its columns. By leveraging the fact that the illumination patterns are uncorrelated, i.e. $\bm{S}\bm{S}^\dagger = \bm{I + \epsilon}$ (where $\bm{I}$ is the identity matrix and $\bm{\epsilon}$ is the statistical noise matrix which diminishes as the number of realizations $M$ increases, eventually becoming negligible with a significant number of such uncorrelated illumination patterns), and the time-reversal symmetry of light propagation ($\bm{P}_{ill}\bm{P}_{ill}^{\dagger}=\bm{I}$), we can analyze the covariance matrix of $\bm{A}$: $\bm{R}=\bm{A}\bm{A}^{\dagger}$, as follows:
            \begin{eqnarray}
            \bm{R}=\bm{A}\bm{A}^\dagger = \bm{P}_{det}\bm{O}\bm{P}_{ill}\bm{S} \bm{S}^\dagger\bm{P}_{ill}^{\dagger} \bm{O}^\dagger \bm{P}^\dagger_{det} \nonumber \\ \approx \bm{P}_{det}\bm{O}_{eff} \bm{P}^\dagger_{det} = \bm{P}_{det}\bm{O}_{eff} (\bm{P}^{*}_{det})^T
            \label{eq:2}
            \end{eqnarray}
            
            where $\bm{O}_{eff} \equiv |\bm{O}|^2$ (see Supplementary S1 for further explanations). 
            
            Remarkably, as seen from Eq. 2, the covariance matrix $\bm{R}$ is mathematically analogous to the conventional reflection matrix from the camera plane to the object and back, when the target object is replaced by $\bm{O}_{eff}$. Hence, one can apply any conventional reflection-matrix analysis approach such as CLASS \cite{kang17,lee22} or distortion matrix \cite{badon2020distortion} on $\bm{R}$.
            
            We chose to apply the recently introduced memory-efficient version of the CTR-CLASS algorithm, whose code is publicly available \cite{weinberg2024noninvasive}, since it allows the application of the CLASS algorithm directly on high pixel-count holographic datasets without the need to explicitly compute the memory-demanding reflection matrix $\bm{R}$
 
        \end{quote}

        \textbf{This is the part copied from, and has to be added to the supplementary:}
        \begin{quote}
            \section{CTR-CLASS}

            The linear nature of a coherent imaging system and the wave equation makes choosing a basis consisting of independent illumination patterns possible. By writing the matrix of the system in this basis (the Green's function), we can fully describe the imaging process. Furthermore, if this matrix is pre-measured, one can obtain the output for any given illumination pattern by multiplying the matrix with the corresponding pattern \cite{popoff2010measuring}.
            For instance, one possible choice for the basis is the set of point-illuminations, which aligns with the description of the imaging system in real space ($\vec{r}$-basis), such that using controlled illumination field $\vec{E}_{in} (\vec{r})$ the output field ${\vec{E}_{out}}(\vec{r})$ can be described as:
            \begin{eqnarray}
            \vec{E}_{out} = \bm{R}_{sys}\vec{E}_{in}
            \label{eq:1}
            \end{eqnarray}
            Conceptually, we can decompose the system's matrix into a product of three distinct matrices, each corresponding to a distinct phase in the imaging process:
            
            \begin{eqnarray}
            \bm{R}_{sys} = \bm{P}_{det}\bm{O}\bm{P}_{ill}
            \label{eq:2}
            \end{eqnarray}
            The first matrix $\bm{P}_{ill}$ represents the propagation through the optical system of the input field to the object plane. The second matrix $\bm{O}$ characterizes the interaction with the object, where this matrix is modeled as a diagonal matrix for thin objects, with the object reflection function $O(\vec{r})$ on its diagonal. The third matrix $\bm{P}_{det}$ governs the propagation of back from the object plane through the optical system to the detection. 
            
            However, due to the very high number of modes in most common systems, measuring the reflection matrix can be daunting and time-consuming. In many practical imaging scenarios, such comprehensive measurements may be unnecessary for imaging within the optical memory effect. Recognizing this, the CTR framework \cite{lee22} presents an alternative approach that exploits the time-reversal symmetry of the illumination process for a phase-only distortion - which translates into the fact the matrix $\bm{P}_{ill}$, which represents the illumination step, is approximated as a unitary matrix $(\bm{P}_{ill}\bm{P}^\dagger_{ill} \approx \mathbf{I})$.
            Instead of measuring the reflection matrix $\bm{R}_{sys}$ in an orderly set of modes, such as the point illumination basis or plane waves basis (in k-space), it is suggested to illuminate a set of random, uncorrelated illumination patterns.
            
            The $M$ random illuminations used in the CTR methodology can be arranged in a matrix $\bm{S}$ in the $\vec{r}$-basis, where each illumination pattern corresponds to a matrix column. Due to their uncorrelated nature, the matrix \textbf{S} approximately satisfies the property $\bm{S} \bm{S}^\dagger = \mathbf{I} + \mathbf{\epsilon} \approx \mathbf{I}$, where $\bm{I}$ is the identity matrix and $\mathbf{\epsilon}$ represents the statistical noise matrix, which diminishes as the number of realizations $M$ increases and becomes negligible with a sufficient number of such realizations.
            
            
            
            
            One can order the $M$ measurements in the columns of 'measurement matrix' $\bm{A}$ which can be written by:
            \begin{eqnarray}
            \bm{A} = \bm{R}_{sys}\bm{S} = \bm{P}_{det}\bm{O}\bm{P}_{ill}\bm{S}
            \label{eq:3}
            \end{eqnarray}
            Then, rather than directly examining the reflection matrix $\bm{R}_{sys}$, the algorithm focuses on the matrix $\bm{R} \stackrel{\text{def}} = \bm{A}\bm{A}^\dagger$, which, from our analysis above can be written by:
            \begin{eqnarray}
            \bm{R}=\bm{A}\bm{A}^\dagger = \bm{P}_{det}\bm{O}\bm{P}_{ill}\bm{S} \bm{S}^\dagger\bm{P}_{ill}^\dagger \bm{O}^\dagger \bm{P}^\dagger_{det} \approx \bm{P}_{det}\bm{O}_{eff} \bm{P}^\dagger_{det} = \bm{P}_{det}\bm{O}_{eff} (\bm{P}^{*}_{det})^T
            \label{eq:4}
            \end{eqnarray}
            where $\bm{O}_{eff} \stackrel{\text{def}} = \bm{O}\bm{O}^\dagger$ and has $|{O}(\vec{r})|^2$ on its diagonal, as $\bm{O}$ is diagonal matrix as we consider thin objects.
            
            This matrix $\bm{R}$ has a similar structure to a full system reflection matrix in the real space ($\vec{r}$-basis), with the same illumination and detection transfer matrix up to phase conjugation.
            In the context of imaging through a scattering medium within the memory-effect regime, we can further model the detection transfer matrix $ \bm{P}_{det}$ as a convolution Toeplitz matrix that represents the distorted point spread function (PSF) any point on the object is convoluted with.
            
            The CTR-CLASS algorithm takes advantage of this unique structure of matrix $\bm{R}$ and estimates $\bm{P}_{det}$ and $\bm{O}_{eff}$ using the CLASS algorithm \cite{kang17}. This is achieved by iteratively leveraging the shifted inter-column correlations within $\bm{R}$.
            A recent improvement introduced a memory-efficient version of the CTR-CLASS algorithm, significantly reducing memory usage to $O(MN)$, where $N$ denotes the number of camera pixels \cite{weinberg2024noninvasive}. This advancement enables the application of the algorithm to megapixel-scale images without the need to explicitly compute $\bm{R}$, which is sized $O(N^2)$.
            It's important to note that while any matrix-based scattering compensation method could theoretically be applied given $\bm{R}$, in this work, we specifically utilize the CLASS algorithm.
            
            \newpage
            \section{CTR-CLASS correction in a conjugated plane}
            As described above, the matrix $\bm{R}$ can be looked at as a reflection matrix for a system:
            \begin{eqnarray}
            \bm{R} = \bm{P}_{det}\bm{O}_{eff} (\bm{P}^{*}_{det})^T
            \label{eq:5}
            \end{eqnarray}
            
            Where for an 'isoplanatic' patch where the distortion does not rely on object coordinates, the matrix $\bm{P}_{det}(\vec{r}_{diff},\vec{r}_{obj})$ describes a transfer matrix from each point on the detection scattering phase mask $\vec{r}_{diff}$ is transmitted towards the object plane with coordinates $\vec{r}_{obj}$. Hence, the effective field in the effective object plane is: $\vec{E}_{obj} = \bm{P}_{det}\vec{E}_{in}$. Assuming free-space configuration and that the distance $d$ between the object and the scattering layer is large enough to apply the Fresnel diffraction approximation, we can derive given an input field ${E}_{in}(x,y)$ the field at the object plane:
            \begin{eqnarray}
            {E}_{obj}(x,y)= \frac{e^{\frac{i2 \pi d}{\lambda}}}{i\lambda d} \iint {E}_{in}(x',y') 
            e^{\frac{-i\pi}{\lambda d} [ (x-x')^2+(y-y')^2 ] } dx'dy'
            \label{eq:6}
            \end{eqnarray}
            where $\lambda$ is the field wavelength. Thus, in a similar way to Choi et al. \cite{choi2022flexible}, each element of $\bm{R}(\vec{r}_{in},\vec{r}_{out})$ can be described as an input field of a point-illumination at $\vec{r}_{in}$ from the diffuser plane and point-detection $\vec{r}_{out}$ at the diffuser plane:
            \begin{eqnarray}
            {E}_{in}(x,y)= e^{-i\phi(x_{in},y_{in})}\delta(x-x_{in},y-y_{in})
            \label{eq:7}
            \end{eqnarray}
            Where $\phi(x_{in},y_{in})$ is the phase distortion of the effective phase-mask and the sign is conjugated from Eq.~\ref{eq:5}. Using Eq.~\ref{eq:7} we obtain:
            \begin{eqnarray}
            {E}_{obj}(x,y) \propto  e^{-i\phi(x_{in},y_{in})}
            e^{\frac{-i\pi}{\lambda d} [(x-x_{in})^2+(y-y_{in})^2 ]}
            \label{eq:8}
            \end{eqnarray}
            Upon reflection from the effective object $O_{eff}$, the detection transfer matrix is applied:
            \begin{align}
            & {E}_{out}(x_{out},y_{out}) = e^{i\phi(x_{out},y_{out})} \frac{e^{\frac{i2\pi d}{\lambda}}}{i\lambda d} \iint {E}_{obj}(x',y')O_{eff}(x',y') 
            e^{\frac{i\pi}{\lambda d} [ (x_{out}-x')^2+(y_{out}-y')^2 ] } dx'dy'  \nonumber &\\ 
            &\propto e^{i\phi(x_{out},y_{out})} \iint e^{\frac{-i\pi}{\lambda d} [(x'-x_{in})^2+(y'-y_{in})^2 ]} O_{eff}(x',y') 
            e^{\frac{i\pi}{\lambda d} [ (x_{out}-x')^2+(y_{out}-y')^2 ] } dx'dy'
             \nonumber &\\ 
            &= e^{i[\phi(x_{out},y_{out})-\phi(x_{in},y_{in})]}e^{\frac{i \pi}{\lambda d}(x^2_{out}+y^2_{out}-x^2_{in}-y^2_{in})} \iint O_{eff}(x',y')e^{-i\frac{2 \pi}{\lambda d}[(x'(x_{out}-x_{in})+y'(y_{out}-y_{in})]}dx'dy'\nonumber &\\ 
            &\equiv e^{i\phi_{eff}(x_{out},y_{out})}\tilde{O}_{eff}(\frac{x_{out}-x_{in}}{\lambda d},\frac{y_{out}-y_{in}}{\lambda d}) e^{-i\phi_{eff}(x_{in},y_{in})}&
            \label{eq:9}
            \end{align}
            
            Where $\phi_{eff}(x,y) \stackrel{\text{def}} = \phi(x,y) + \frac{\pi}{\lambda d}(x^2+y^2)$ and $\tilde{O}_{eff}(\nu_x,\nu_y)$ is the Fourier spectrum of ${O}_{eff}$, leading to a scaled ${O}_{eff}$ with a scaling factor of ${\lambda d}$. 
            
            So $\bm{R}(\vec{r}_{in},\vec{r}_{out})$ has a structure of a matrix similar to an isoplanatic imaging object beyond scattering layers system reflection matrix in the $\vec{k}$-basis. This matrix format aligns with the input requirements for the CLASS algorithm \cite{kang17,lee22}.

        \end{quote}
\end{comment}




%--------------------------------------------------------------------
% Supplementary Section – Algorithmic Foundations of
% CLASS, CTR‑CLASS and Dynamic‑CLASS
%--------------------------------------------------------------------
\section{Matrix‑based scattering compensation algorithms}
\label{supsec:algorithms}

%====================================================================
\subsection{1.\quad Single illumination: reflection‑matrix derivation}
\label{subsec:R_derivation}

\paragraph{Definitions.}~\\
Assuming a thin aberrating layer in front of a reflective object that lies
within the isoplanatic patch, we denote
\begin{itemize}
  \item $E_{\text{in}}(\mathbf r)$ – complex field impinging on the layer,
  \item $P_{\text{ill}}(\mathbf r)$ – point‑spread function (PSF) for the forward path,
  \item $O(\mathbf r)$ – complex reflectivity of the object,
  \item $P_{\text{det}}(\mathbf r)$ – PSF for the return path,
  \item $E_{\text{out}}(\mathbf r)$ – field reaching the detector.
\end{itemize}

\paragraph{Field equation.}~\\
The round‑trip field is
\begin{equation}
E_{\text{out}}(\mathbf r)=
P_{\text{det}} \ast
\Bigl\{
  O(\mathbf r)\,
  \bigl[P_{\text{ill}}\ast E_{\text{in}}\bigr](\mathbf r)
\Bigr\},
\label{eq:doublepass_sup}
\end{equation}
where “$\ast$” denotes 2‑D convolution in the transverse coordinate
$\mathbf r$.

\paragraph{Discretisation and matrix form.}~\\
Sampling on an $L\times L$ grid ($N=L^{2}$ pixels) and vectorising each field
into an $N\times1$ column, $e_{\text{in}}$ or $e_{\text{out}}$, converts
Eq.~\eqref{eq:doublepass_sup} to
\begin{equation}
e_{\text{out}}=
P_{\text{det}}\;
O\;
P_{\text{ill}}\;
e_{\text{in}},
\label{eq:R_matrix_form}
\end{equation}
where
\begin{enumerate}
  \item $P_{\text{ill}}$ – $N\times N$ Toeplitz (convolution) matrix,
  \item $O=\mathrm{diag}(O_{1},\dots,O_{N})$ – diagonal object matrix,
  \item $P_{\text{det}}$ – $N\times N$ Toeplitz convolution matrix.
\end{enumerate}

\noindent\textbf{Multiple illuminations.}~\\
Equation~\eqref{eq:R_matrix_form} is written for a \emph{single}
illumination field.  If we illuminate the sample with $M$ different fields,
we stack them as the columns of
\(E_{\text{in}}\in\mathbb{C}^{N\times M}\).
The corresponding detected fields form
\(E_{\text{out}}\in\mathbb{C}^{N\times M}\),
and Eq.~\eqref{eq:R_matrix_form} generalises to
\[
E_{\text{out}} = P_{\text{det}}\,O\,P_{\text{ill}}\,E_{\text{in}}.
\]
Thus both $E_{\text{in}}$ and $E_{\text{out}}$ are $N\times M$ matrices,
each \emph{column} containing the input or output field for one
illumination.
\paragraph{Reflection atrix}~\\
Collecting the operators that map the incident field to the detected field,
we define the \emph{reflection matrix}
\begin{equation}
R \;=\; P_{\text{det}}\,
        O\,
        P_{\text{ill}},
\qquad\Longrightarrow\qquad
e_{\text{out}} = R\,e_{\text{in}}.
\label{eq:R_def}
\end{equation}
\paragraph{CLASS decomposition.}~\\
\(R\) is the product of a Toeplitz, a diagonal, and a Toeplitz matrix,
the CLASS algorithm can exploit this specific structure to iteratively
factor \(R\) into its three components \(P_{\text{ill}}\), \(P_{\text{det}}\),
and \(O\), thereby recovering the aberration‑free object $O$.

%====================================================================
\subsection{2.\quad CTR/I‑CLASS: random‑illumination generalisation}
\label{subsec:ctr_class}

Illuminating the sample $m=1,\dots,M$ times with random, uncorrelated
illumination patterns yields the imaging equation
\begin{equation}
I_{m}(\mathbf r)
  = P(\mathbf r)\ast O_{m}(\mathbf r).
\label{imaging_eq}
\end{equation}

Here $O_m$ is the object under the $m^{\text{th}}$ illumination.  For the
matrix representation we collect the random illumination patterns in the
columns of $S_{\text{ill}}\in\mathbb{C}^{N\times M}$.
Defining $S = P_{\text{ill}} S_{\text{ill}}$ and stacking the detected
fields column‑wise gives the measurement matrix
\begin{equation}
A \;=\; P_{\text{det}}\,
        O\,
        P_{\text{ill}}\,
        S_{\text{ill}}
     =\; P_{\text{det}}\,
        O\,
        S.
\label{eq:A_def}
\end{equation}
The covariance matrix is
\begin{equation}
\hat{A}\,\hat{A}^{\dagger}
      = P_{\text{det}}\,
        O\,
        \bigl(\hat{S}\,\hat{S}^{\dagger}\bigr)\,
        O^{\dagger}\,
        P_{\text{det}}^{\dagger}.
\label{eq:A_cov}
\end{equation}
For uncorrelated illumination it can be shown [citations] that
$\hat{S}\hat{S}^{\dagger}\propto I$, leading to
\begin{equation}
\hat{A}\,\hat{A}^{\dagger}
      = P_{\text{det}}\,
        O\,O^{\dagger}\,
        P_{\text{det}}^{\dagger}
      = P_{\text{det}}\,
        |O|^{2}\,
        P_{\text{det}}^{\dagger}.
\label{eq:A_cov2}
\end{equation}
This restores the same TDT form as Eq.~\eqref{eq:R_def}, so CLASS operates
on $\hat{A}\hat{A}^{\dagger}$ exactly as on $R$, constituting
CTR‑CLASS/I‑CLASS.

%====================================================================
\subsection{3.\quad Dynamic scattering: exchanging roles of object and PSF}
\label{subsec:dyn_class}

With rapidly varying detection PSFs $P_{m}$ and a static object $O$, each
short‑exposure frame obeys
\[
I_{m}(\mathbf r)
  = P_{m}(\mathbf r)\ast O(\mathbf r)
  = O(\mathbf r)\ast P_{m}(\mathbf r).
\]
(We neglect $P_{\text{ill}}$: in the coherent case we showed that for a
small spot the illumination is unaffected by the varying scattering
(see~...), and in the incoherent case multiple speckle illuminations are
summed within each realisation of the scattering medium so that the
effective illumination is constant (see~...).)

Thus we obtain the same imaging‑equation form as in
Eq.~\eqref{imaging_eq} and can apply exactly the same algorithm used for
the random‑illumination case (CTR‑CLASS/I‑CLASS).

%=




\bibliographystyle{plain}
\bibliography{bib}

\end{document}


